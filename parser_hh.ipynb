{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "def get_all_vacancies(search_query=\"–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö\", region=113, max_vacancies=2000):\n",
        "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤\"\"\"\n",
        "    base_url = \"https://api.hh.ru/vacancies\"\n",
        "    all_vacancies = []\n",
        "    page = 0\n",
        "    per_page = 100\n",
        "\n",
        "    # –ï—Å–ª–∏ —Ä–µ–≥–∏–æ–Ω –ø–µ—Ä–µ–¥–∞–Ω –∫–∞–∫ —Å–ø–∏—Å–æ–∫, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É\n",
        "    area_ids = region if isinstance(region, list) else [region]\n",
        "\n",
        "    print(f\"üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{search_query}'\")\n",
        "\n",
        "    for area_id in area_ids:\n",
        "        print(f\"üåç –†–µ–≥–∏–æ–Ω: {get_region_name(area_id)} (ID: {area_id})\")\n",
        "        page = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞\n",
        "\n",
        "        while True:\n",
        "            params = {\n",
        "                \"text\": search_query,\n",
        "                \"area\": area_id,\n",
        "                \"page\": page,\n",
        "                \"per_page\": per_page,\n",
        "                \"search_field\": \"name\"  # –ò—â–µ–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(base_url, params=params)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "\n",
        "                if not data.get(\"items\"):\n",
        "                    print(f\"  ‚û§ –í–∞–∫–∞–Ω—Å–∏–∏ –≤ —Ä–µ–≥–∏–æ–Ω–µ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å\")\n",
        "                    break\n",
        "\n",
        "                all_vacancies.extend(data[\"items\"])\n",
        "                print(f\"  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page+1}: +{len(data['items'])} –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: {len(all_vacancies)})\")\n",
        "\n",
        "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞\n",
        "                if len(all_vacancies) >= max_vacancies or page >= data[\"pages\"] - 1:\n",
        "                    break\n",
        "\n",
        "                page += 1\n",
        "                time.sleep(0.2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}\")\n",
        "                break\n",
        "\n",
        "            if len(all_vacancies) >= max_vacancies:\n",
        "                print(f\"üö© –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_vacancies} –≤–∞–∫–∞–Ω—Å–∏–π\")\n",
        "                break\n",
        "\n",
        "    return all_vacancies[:max_vacancies]\n",
        "\n",
        "def get_region_name(region_id):\n",
        "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ä–µ–≥–∏–æ–Ω–∞ –ø–æ ID\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"https://api.hh.ru/areas/{region_id}\")\n",
        "        return response.json()[\"name\"]\n",
        "    except:\n",
        "        return f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–≥–∏–æ–Ω ({region_id})\"\n",
        "\n",
        "def clean_html(html_text):\n",
        "    \"\"\"–û—á–∏—Å—Ç–∫–∞ HTML-—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    if not html_text:\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    return soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "def parse_vacancies(vacancies):\n",
        "    \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö\"\"\"\n",
        "    parsed_data = []\n",
        "\n",
        "    print(\"\\nüß† –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "    for idx, vacancy in enumerate(vacancies, 1):\n",
        "        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
        "        salary = vacancy.get(\"salary\") or {}\n",
        "\n",
        "        # –î–µ—Ç–∞–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–∏\n",
        "        details = get_vacancy_details(vacancy[\"id\"]) if vacancy.get(\"id\") else {}\n",
        "\n",
        "        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã\n",
        "        published_at = vacancy.get(\"published_at\")\n",
        "        if published_at:\n",
        "            published_at = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%S%z\").strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞\n",
        "        address = details.get(\"address\") or {}\n",
        "        address_str = \", \".join(filter(None, [\n",
        "            address.get(\"city\"),\n",
        "            address.get(\"street\"),\n",
        "            address.get(\"building\")\n",
        "        ]))\n",
        "\n",
        "        parsed_data.append({\n",
        "            \"id\": vacancy.get(\"id\"),\n",
        "            \"–Ω–∞–∑–≤–∞–Ω–∏–µ\": vacancy.get(\"name\"),\n",
        "            \"–∫–æ–º–ø–∞–Ω–∏—è\": vacancy.get(\"employer\", {}).get(\"name\"),\n",
        "            \"–∑–∞—Ä–ø–ª–∞—Ç–∞_–æ—Ç\": salary.get(\"from\"),\n",
        "            \"–∑–∞—Ä–ø–ª–∞—Ç–∞_–¥–æ\": salary.get(\"to\"),\n",
        "            \"–≤–∞–ª—é—Ç–∞\": salary.get(\"currency\"),\n",
        "            \"–æ–ø—ã—Ç\": vacancy.get(\"experience\", {}).get(\"name\"),\n",
        "            \"–∑–∞–Ω—è—Ç–æ—Å—Ç—å\": vacancy.get(\"employment\", {}).get(\"name\"),\n",
        "            \"–≥—Ä–∞—Ñ–∏–∫\": vacancy.get(\"schedule\", {}).get(\"name\"),\n",
        "            \"—Å—Å—ã–ª–∫–∞\": vacancy.get(\"alternate_url\"),\n",
        "            \"–¥–∞—Ç–∞_–ø—É–±–ª–∏–∫–∞—Ü–∏–∏\": published_at,\n",
        "            \"–æ–ø–∏—Å–∞–Ω–∏–µ\": clean_html(details.get(\"description\", \"\")),\n",
        "            \"–Ω–∞–≤—ã–∫–∏\": \", \".join([s[\"name\"] for s in details.get(\"key_skills\", [])]),\n",
        "            \"—Ä–µ–≥–∏–æ–Ω\": details.get(\"area\", {}).get(\"name\"),\n",
        "            \"–∞–¥—Ä–µ—Å\": address_str,\n",
        "            \"—Ç–∏–ø_–∫–æ–º–ø–∞–Ω–∏–∏\": vacancy.get(\"employer\", {}).get(\"type\")\n",
        "        })\n",
        "\n",
        "        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
        "        if idx % 50 == 0:\n",
        "            print(f\"  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx}/{len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π\")\n",
        "\n",
        "    return parsed_data\n",
        "\n",
        "def get_vacancy_details(vacancy_id):\n",
        "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏–∏\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"https://api.hh.ru/vacancies/{vacancy_id}\")\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "def create_vacancies_table(data):\n",
        "    \"\"\"–°–æ–∑–¥–∞–µ–º DataFrame —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏\"\"\"\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
        "    df[\"–∑–∞—Ä–ø–ª–∞—Ç–∞_–æ—Ç\"] = pd.to_numeric(df[\"–∑–∞—Ä–ø–ª–∞—Ç–∞_–æ—Ç\"], errors=\"coerce\")\n",
        "    df[\"–∑–∞—Ä–ø–ª–∞—Ç–∞_–¥–æ\"] = pd.to_numeric(df[\"–∑–∞—Ä–ø–ª–∞—Ç–∞_–¥–æ\"], errors=\"coerce\")\n",
        "    df[\"–¥–∞—Ç–∞_–ø—É–±–ª–∏–∫–∞—Ü–∏–∏\"] = pd.to_datetime(df[\"–¥–∞—Ç–∞_–ø—É–±–ª–∏–∫–∞—Ü–∏–∏\"], errors=\"coerce\")\n",
        "\n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
        "    initial_count = len(df)\n",
        "    df = df.drop_duplicates(subset=\"id\")\n",
        "    final_count = len(df)\n",
        "\n",
        "    if initial_count != final_count:\n",
        "        print(f\"üîÅ –£–¥–∞–ª–µ–Ω–æ {initial_count - final_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\")\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–±–æ—Ä–∞\n",
        "    KEYWORD = \"–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö\"  # –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
        "    REGIONS = [113]  # 113 = –†–æ—Å—Å–∏—è, 1 = –ú–æ—Å–∫–≤–∞, 2 = –°–ü–±\n",
        "    MAX_VACANCIES = 1000\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{KEYWORD}'\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏\n",
        "    raw_vacancies = get_all_vacancies(\n",
        "        search_query=KEYWORD,\n",
        "        region=REGIONS,\n",
        "        max_vacancies=MAX_VACANCIES\n",
        "    )\n",
        "\n",
        "    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ\n",
        "    parsed_data = parse_vacancies(raw_vacancies)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
        "    df = create_vacancies_table(parsed_data)\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    filename = f\"hh_{KEYWORD.replace(' ', '_')}_{timestamp}.csv\"\n",
        "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} –≤–∞–∫–∞–Ω—Å–∏–π\")\n",
        "    print(f\"üíæ –§–∞–π–ª: {filename}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–≤–æ–¥–∫–∞\n",
        "    if not df.empty:\n",
        "        print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "        print(f\"- –°—Ä–µ–¥–Ω—è—è –ó–ü –æ—Ç: {df['–∑–∞—Ä–ø–ª–∞—Ç–∞_–æ—Ç'].mean():.0f} {df['–≤–∞–ª—é—Ç–∞'].mode()[0]}\")\n",
        "        print(f\"- –°—Ä–µ–¥–Ω—è—è –ó–ü –¥–æ: {df['–∑–∞—Ä–ø–ª–∞—Ç–∞_–¥–æ'].mean():.0f} {df['–≤–∞–ª—é—Ç–∞'].mode()[0]}\")\n",
        "        print(f\"- –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –Ω–∞–≤—ã–∫–∏: {', '.join(df['–Ω–∞–≤—ã–∫–∏'].str.split(', ').explode().value_counts().head(5).index.tolist())}\")\n",
        "        print(f\"- –¢–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤: {', '.join(df['—Ä–µ–≥–∏–æ–Ω'].value_counts().head(3).index.tolist())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch5GnEdYIjJt",
        "outputId": "5e9ad38e-8c00-4d40-a049-2c0b54b872c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö'\n",
            "======================================================================\n",
            "üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∑–∞–ø—Ä–æ—Å—É: '–ê–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö'\n",
            "üåç –†–µ–≥–∏–æ–Ω: –†–æ—Å—Å–∏—è (ID: 113)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 1: +100 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 100)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 2: +100 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 200)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 3: +100 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 300)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 4: +100 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 400)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 5: +100 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 500)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 6: +100 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 600)\n",
            "  ‚û§ –°—Ç—Ä–∞–Ω–∏—Ü–∞ 7: +33 –≤–∞–∫–∞–Ω—Å–∏–π (–≤—Å–µ–≥–æ: 633)\n",
            "\n",
            "üß† –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 50/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 100/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 150/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 200/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 250/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 300/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 350/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 400/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 450/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 500/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 550/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "  ‚û§ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 600/633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "\n",
            "======================================================================\n",
            "‚úÖ –°–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ 633 –≤–∞–∫–∞–Ω—Å–∏–π\n",
            "üíæ –§–∞–π–ª: hh_–ê–Ω–∞–ª–∏—Ç–∏–∫_–¥–∞–Ω–Ω—ã—Ö_20250802_1901.csv\n",
            "======================================================================\n",
            "\n",
            "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
            "- –°—Ä–µ–¥–Ω—è—è –ó–ü –æ—Ç: 97250 RUR\n",
            "- –°—Ä–µ–¥–Ω—è—è –ó–ü –¥–æ: 134270 RUR\n",
            "- –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –Ω–∞–≤—ã–∫–∏: , SQL, Python, –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö, MS Excel\n",
            "- –¢–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤: –ú–æ—Å–∫–≤–∞, –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ö–∞–∑–∞–Ω—å\n"
          ]
        }
      ]
    }
  ]
}